\section{Introduction}

Parallel linear solvers aim to solve large systems of equations on multi-core and/or multi-CPU computer architectures. From desktop workstations with a single CPU with many cores, to supercomputers with hundreds of racks of nodes yielding thousands of CPUs, modern high performance computing systems provide significant compute power for numerical methods. The algorithms employed across these machines take advantage of massive compute power by distributing the workload across all compute resources. Developing parallel algorithms for solving partial differential equations on supercomputers has an active area of research, even prior to the deployment of modern clusters \citep{ortega1985solution}.

Domain decomposition methods are popular approaches to solving elliptic partial differential equations in parallel \citep{smith1997domain}. By partitioning the domain, each compute unit can independently work towards the solution of the global problem by solving the problem on a subset of the entire domain. The global solution can be constructed by equating the solution and/or fluxes across partition boundaries. Communication between compute units is required during this reconstruction. Optimizing the communication between compute units, often through asynchronous techniques \citep{glusa2020scalable} or by reducing the amount of communication, results in more efficient implementations. The memory bandwidth between compute units is often one to several orders of magnitude smaller compared to the bandwidth on the compute unit.

With a partitioned domain, the goal is to solve the elliptic PDE using numerical methods such as finite difference, finite volume, finite element, and more. These discretizations lead to a linear system that must be solved. Ideally, the solvers take advantage of the sparsity and structure of the associated linear system. Parallel linear solvers using iterative or direct methods have been successfully developed and implemented. Parallel iterative methods include GMRES \citep{saad1986gmres}, the conjugate gradient method \citep{hestenes1952methods}, the algebraic multigrid method \citep{yang2002boomeramg}, and the geometric multigrid method \citep{sundar2012parallel} (with a good overview provided in \citep{chow2006survey}). Parallel direct methods include matrix factorization methods like LU-factorization, Cholesky factorization and the spectral value decomposition \citep{donfack2015survey, demmel1999asynchronous, gupta1997highly}.

Several, large-scale and open-source codes currently exist to solve linear systems formed from elliptic PDEs. The hypre library \citep{falgout2002hypre} features scalable preconditioners for parallel multigrid methods. The SuperLU library \citep{li2005superlu} is a general purpose library for solving sparse linear systems using direct methods. Additional codes like PETSc \citep{anl2023petsc}, FLASH-X \citep{dubey2022flash}, and AMReX \citep{zhang2019amrex}, contain iterative and direct solvers that also work with adaptive mesh refinement. The ForestClaw code \citep{calhoun2017forestclaw} coupled with the ThunderEgg repository \citep{aiton2022thunderegg} implements hyperbolic and elliptic solvers for finite volume meshes on adaptively refined quadtrees and octrees provided from \pforest. \pforest\ \citep{burstedde2011p4est,burstedde2020parallel} is a highly scalable AMR code that provides quadtree and octree data structures for users to build on top of. The EllipticForest library \citep{chipman2024ellipticforest} contains an implementation of the quadtree-adaptive HPS method, including the parallel algorithms outlined in this paper. Another code of interest is the ButterflyPACK library \citep{liu2018butterflypack} that solves large-scale dense systems with off-diagonal, low-rank structure like the matrices formed in the HPS method.

The Hierarchical Poincar√©-Steklov (HPS) method \citep{martinsson2015hierarchical, gillman2014direct} is a matrix-free, direct method for solving elliptic PDEs. The goal is to build up a factorization by successively merging subdomains via a class of Poincar\'e-Steklov operators \citep{quarteroni1991theory} called Dirichlet-to-Neumann operators. It is a domain decomposition method that was originally inspired by the partitioning scheme of nested dissection \citep{george1973nested,lipton1979generalized}. The Dirichlet-to-Neumann operators have off-diagonal, low-rank structure that can be exploited to achieve linear $\mathcal{O}(N)$ complexity in the factorization stage \citep{gillman2014direct}. While relatively new, the HPS method has been applied to solve 3D elliptic PDEs \citep{hao2016direct}, has been coupled with the spectral element method \citep{fortunato2020ultraspherical}, and implemented on adaptive meshes \citep{babb2018accelerated, geldermans2019adaptive,chipman2024fast}. Recently, the HPS method has also been implemented in parallel, targeting shared-memory machines \citep{beams2020parallel}, distributed-memory machines \citep{yesypenko2022parallel}, and GPU devices \citep{yesypenko2022gpu}.

The quadtree-adaptive HPS method presented in \citep{chipman2024fast} is an implementation of the HPS method on adaptively refined quadtree meshes provided from \pforest \citep{burstedde2011p4est,burstedde2020parallel}. \pforest\ provides efficiently partitioned quadtree data structures for the quadtree-adaptive HPS method. The novelty of this paper is the porting of the quadtree-adaptive HPS method in parallel, building on top of the parallel infrastructure provided from \pforest. This paper will present how the leaf-indexed quadtree data structure provided from \pforest\ is wrapped, the communication patterns associated with building the matrix-free factorization, and scaling analysis and discussion.

% \subsection{Paper Overview}

As an overview for this paper, \refsec{sec:overview-of-the-quadtree-adaptive-hps-method} provides an overview of the quadtree-adaptive HPS method, including the problem description and algorithms. \refsec{sec:parallel-algorithm} describes the parallel implementation and details the data structures and algorithms necessary for distributed-memory parallelism. \refsec{sec:parallel-results-and-discussion} contains the results and a discussion of the implementation presented herein.