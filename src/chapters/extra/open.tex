\section{Open Research Questions}
\label{sec:open}

There remain topics for potential research in the area of fast solvers for elliptic PDEs. In this paper, we looked at classic approaches to approximating solutions of PDEs including finite difference, finite volume, finite element, and spectral methods. Each of these approaches have been analyzed and expanded, with high order methods and fast and parallel algorithms being the primary focus of recent advancements. Although other discretization methods such as collocation methods and meshless methods remain an ongoing research topic, we will look at some open research topics in the area of solution methods, as it is an area with more potential questions and applications. We have already addressed classic solution methods, which we categorized as iterative, direct, or hierarchical. Iterative methods such as splitting or Krylov methods and direct methods such as matrix factorizations are well established in the literature, with textbooks and courses addressing these topics. Indeed, these topics form the foundation for numerical analysis for PDEs. The methods we classified as hierarchical methods (i.e., the multigrid method, nested dissection, and the HPS method) are more open topics. Multigrid methods remain a popular solution technique for a variety of discretization methods, as well as on adaptive meshes (\citep{babich2010adaptive,thompson1989adaptive}). The HPS method has also been introduced and is being improved upon, as in (\citep{fortunato2020ultraspherical,geldermans2019adaptive,gillman2014direct,martinsson2015hierarchical}). However, open questions exist with regard to parallel implementations and further applications of these methods.

Applying the multigrid or HPS method to a particular problem opens up a wide range of new ideas. For example, in the area of computational fluid dynamics (CFD), the multigrid method has been used to help solve the Navier-Stokes equations (\citep{babich2010adaptive}). Wherever the multigrid method has been applied, the HPS method can also be applied for some competitive results. It would be of interest to see how the HPS method and multigrid method compare when applied to the same problem with the same discretization scheme. For example, the multigrid method has been applied to the Serre-Green-Naghdi model with a quadtree discretization (\citep{popinet2015quadtree}). The HPS method works well on quadtrees and can also be applied here, with the advantage of a direct method for faster solutions.

Both the multigrid and HPS method have potential for high scalability. The multigrid method has been implemented in parallel in a variety of papers (\citep{bergen2006massively}). The main approach is to use a domain decomposition approach to divide the domain among processors. The HPS method, however, is fairly new and has limited study in parallelization. Beams et al. in \citep{beams2020parallel} describe a shared-memory implementation of the HPS method. However, they do not address distributed memory paradigms or implementations on a GPU. Additionally, while their results show promising scaling results, they use a relatively small number of cores (up to 28 physical cores). It would be of interest to see how the HPS method scales with communication cost and on much larger machines using a distributed memory paradigm or heterogeneous architecture. The motivation for highly scalable algorithms comes from the advances in high performance computing. Modern supercomputers have millions of cores, and their speed comes from the distribution of work. State of the art supercomputers need to use advanced algorithms that scale well enough to take advantage of the compute power. This is the mission of the United States' Department of Energy's (DoE) Exascale Computing Project (ECP); to combine exascale-capable machines with highly scalable algorithms for ``codesign, modeling and simulation, data analytics, machine learning, and artificial intelligence" (\citep{doe2021exascale}).
